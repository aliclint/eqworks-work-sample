{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "identified-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.\n",
      "Count of data sample before removing duplicates: 22025\n",
      "Count of data sample after removing duplicates: 19999\n",
      "Question 2\n",
      "Since we later need to 'draw' a circle, I have chosen to implement the euclidean distance over the haversine distance.\n",
      "+-------+--------------------+-------+--------+---------------+--------+----------+-----+---------+-----------+------------------+\n",
      "|     ID|           TimeStamp|Country|Province|           City|Latitude| Longitude|POIID|  poi_lat|   poi_long|euclidean_distance|\n",
      "+-------+--------------------+-------+--------+---------------+--------+----------+-----+---------+-----------+------------------+\n",
      "|5110438|2017-06-21 12:41:...|     CA|      AB|   Medicine Hat| 50.0816| -110.5788| POI1|53.546167|-113.485734|4.5225534577100275|\n",
      "|4794397|2017-06-21 16:18:...|     CA|      AB|        Calgary|50.90992|-114.12899| POI1|53.546167|-113.485734|2.7135910757785515|\n",
      "|4886318|2017-06-21 09:26:...|     CA|      AB|        Calgary|50.91459| -114.0219| POI1|53.546167|-113.485734| 2.685641727126872|\n",
      "|5605083|2017-06-21 09:21:...|     CA|      AB|Redwood Meadows| 50.9514| -114.3591| POI1|53.546167|-113.485734|  2.73780641285044|\n",
      "|5182545|2017-06-21 16:46:...|     CA|      AB|        Calgary|50.95462|-114.11521| POI1|53.546167|-113.485734| 2.666900052080131|\n",
      "|5106022|2017-06-21 18:41:...|     CA|      AB|        Calgary| 50.9632|  -114.082| POI1|53.546167|-113.485734|2.6508963891191564|\n",
      "|4741980|2017-06-21 10:14:...|     CA|      AB|        Calgary| 51.0242| -114.1004| POI1|53.546167|-113.485734|2.5957911781661065|\n",
      "|5467677|2017-06-21 19:10:...|     CA|      AB|        Calgary| 51.0272| -114.0349| POI1|53.546167|-113.485734| 2.578134605222346|\n",
      "|5058508|2017-06-21 16:37:...|     CA|      AB|        Calgary| 51.0486|  -113.968| POI1|53.546167|-113.485734| 2.543702304564155|\n",
      "|5266261|2017-06-21 14:52:...|     CA|      AB|        Calgary| 51.0524|  -114.038| POI1|53.546167|-113.485734|2.5541870693128557|\n",
      "|4694723|2017-06-21 12:11:...|     CA|      AB|        Calgary|  51.053|  -114.161| POI1|53.546167|-113.485734| 2.582995520833323|\n",
      "|4661048|2017-06-21 20:08:...|     CA|      AB|        Calgary|  51.053|  -114.161| POI1|53.546167|-113.485734| 2.582995520833323|\n",
      "|5224902|2017-06-21 00:49:...|     CA|      AB|        Calgary|  51.063|  -113.889| POI1|53.546167|-113.485734|2.5156990711619254|\n",
      "|5496560|2017-06-21 18:12:...|     CA|      AB|        Calgary| 51.0823|  -114.142| POI1|53.546167|-113.485734| 2.549769726160581|\n",
      "|4671387|2017-06-21 21:09:...|     CA|      AB|        Calgary| 51.0823| -113.9578| POI1|53.546167|-113.485734|2.5086823039287007|\n",
      "|4534509|2017-06-21 03:00:...|     CA|      AB|        Calgary| 51.0876| -114.0214| POI1|53.546167|-113.485734| 2.516245170297398|\n",
      "|5544642|2017-06-21 14:17:...|     CA|      AB|        Calgary| 51.0905|  -114.182| POI1|53.546167|-113.485734|2.5524667985392098|\n",
      "|5428996|2017-06-21 13:06:...|     CA|      AB|        Calgary| 51.0915| -114.2073| POI1|53.546167|-113.485734|2.5585244914295786|\n",
      "|4878886|2017-06-21 19:25:...|     CA|      AB|        Calgary|51.09722|-113.94464| POI1|53.546167|-113.485734| 2.491573022338498|\n",
      "|4717553|2017-06-21 01:12:...|     CA|      AB|        Calgary| 51.1229|  -114.189| POI1|53.546167|-113.485734| 2.523253063021024|\n",
      "+-------+--------------------+-------+--------+---------------+--------+----------+-----+---------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Check: There are 19999 entries in this dataframe. Then it means we still don't have duplicates after the cross join.\n",
      "Question 3\n",
      "The average and standard deviation between the POI to each of its assigned requests is as follows.\n",
      "Note that POI1 and POI2 are in the same exact location, and in this run POI2 have no requests assigned to it and have null as values.\n",
      "Assumption: Given a POI with duplicate geographical data points, all requests will route to one of the POI with that data points\n",
      "+-----+-----------------+--------------------+------------------+--------------------+\n",
      "|POIID| average_distance|stddev_samp_distance|            radius|             density|\n",
      "+-----+-----------------+--------------------+------------------+--------------------+\n",
      "| POI1|3.348183006325605|   3.858489571570677|24.851937229893878|  1.5909698090373576|\n",
      "| POI2|             null|                null|              null|                null|\n",
      "| POI3|5.537950830488864|   2.858689729540489|20.155377791591437|  2.4484833757802162|\n",
      "| POI4|8.810410862715695|  28.675130269811003| 192.7049913074258|0.001320564715537809|\n",
      "+-----+-----------------+--------------------+------------------+--------------------+\n",
      "\n",
      "Question 4a #1\n",
      "Providing a mathematical model to map the popularity of POIs in a scale of [-10,10].\n",
      "This solution is inspired by the boxplot way of visualizing data where we can see the centrality of the data based on the median which is not sensitive to outliers.\n",
      "In essence we have two calculated fields which is the count of cities over province and density for each POI.\n",
      "Number of countries was ommited because I the data comes from Canada only (Validated in bonus.txt).\n",
      "Then we calculate the percentile rank over each of the rows. The percentile method will take care of the new requests coming in, that can be outliers, since it depends on the data itself and will scale accordingly.\n",
      "We sum the percentiles of the calculated field (each calculated field have ranges [0,1]), and we get a data value of range [0,2]. We multiply by 10 and subtract by 10 to get the desired scale of [-10,10].\n",
      "Note that POI2 is considered as popularity -10 as expected since no requests are ever routed there in this iteration.\n",
      "Equations:\n",
      "city_over_province = count(city)/count(province)\n",
      "density = requests / area\n",
      "Note: In order to scale this process perhaps recalculating the popularity over a period of time daily or 4x a day depending on the density of requests of the time and day.\n",
      "popularity = (percent_rank(city_over_province) + percent_rank(density))*10 - 10\n",
      "+-----+---------------------------+------------------+------------------+\n",
      "|POIID|city_over_province_pct_rank|  density_pct_rank|        popularity|\n",
      "+-----+---------------------------+------------------+------------------+\n",
      "| POI2|                        0.0|               0.0|             -10.0|\n",
      "| POI4|         0.3333333333333333|0.3333333333333333|-3.333333333333334|\n",
      "| POI1|         0.6666666666666666|0.6666666666666666| 3.333333333333332|\n",
      "| POI3|                        1.0|               1.0|              10.0|\n",
      "+-----+---------------------------+------------------+------------------+\n",
      "\n",
      "Question 4b\n",
      "We are using the graphframes package that is better suited to work with graphs.\n",
      "Need to find the minimum amount of tasks that are ordered based on the dag's topology.\n",
      "We begin by finding the shortest path from task 73 to task 36.\n",
      "Then we would do a postorder traversal when printing the tasks. I.e. dependent task then parent task.\n",
      "This is our shortest path from the task start to task end: ['73', '20', '97', '36']\n",
      "We need to find all the dependent tasks in between each task excluding the task start.\n",
      "These are all the tasks needed to be completed starting from from start to end: ['73', '21', '100', '20', '112', '97', '94', '56', '102', '36']\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType \n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def findPrerequisiteTask(g, task, visited):\n",
    "    \"\"\"\n",
    "    Returns the prerequisite tasks of the current task id.\n",
    "\n",
    "    Keyword arguments:\n",
    "    g -- GraphFrames object with id,dst,src\n",
    "    task -- list of string of length 1 that contains the current task id\n",
    "    visited -- list of string that contains previously visited vertices\n",
    "    \"\"\"\n",
    "    sol = []\n",
    "    prerequisite_task = g.shortestPaths(task)\n",
    "    prerequisite_task = prerequisite_task.sort([\"id\"]).select(\"id\", \"distances\").select(\"id\", explode(\"distances\")).orderBy(\"value\",ascending=True)\n",
    "    prerequisite_task = prerequisite_task.filter(~prerequisite_task.id.isin(visited)).select(\"id\").filter(\n",
    "        prerequisite_task.value > 0).select(\"id\").rdd.flatMap(lambda x: x).collect()\n",
    "    for pre_req_task in prerequisite_task:\n",
    "        if pre_req_task not in visited:\n",
    "            temp_sol, temp_visited = findPrerequisiteTask(g, [pre_req_task], visited + [pre_req_task])\n",
    "            sol = temp_sol + sol\n",
    "            visited = temp_sol + visited\n",
    "    visited = task + visited\n",
    "    sol = sol + task  \n",
    "    return sol, visited\n",
    "\n",
    "\n",
    "def findInitialVisited(g, task):\n",
    "    \"\"\"\n",
    "    Returns the initial visited nodes to help filter out the results in the recursion.\n",
    "\n",
    "    Keyword arguments:\n",
    "    g -- GraphFrames object with id,dst,src\n",
    "    task -- list of string of length 1 that contains the current task id\n",
    "    visited -- list of string that contains previously visited vertices\n",
    "    \"\"\"\n",
    "    visited = []\n",
    "    prerequisite_task = g.shortestPaths(task)\n",
    "    prerequisite_task = prerequisite_task.sort([\"id\"]).select(\"id\", \"distances\").select(\"id\", explode(\"distances\")).orderBy(\"value\",ascending=True)\n",
    "    prerequisite_task = prerequisite_task.filter(prerequisite_task.value > 0).select(\"id\").rdd.flatMap(lambda x: x).collect()\n",
    "    visited = visited + task + prerequisite_task\n",
    "    return visited\n",
    "\n",
    "def readQuestion(pathToFile):\n",
    "    \"\"\"\n",
    "    Reads in a .txt file and returns a list  of length 2 in the form of [[start_task_id], end_task_id]\n",
    "    \n",
    "    Example:\n",
    "    questions.txt\n",
    "    starting task: 73\n",
    "    goal task: 36\n",
    "    -> returns [[73],36]\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    with open(pathToFile, \"r\") as file:\n",
    "        for cnt, line in enumerate(file):\n",
    "            targetId = line.split(\": \")[1]\n",
    "            targetId = targetId.strip().split(\",\")\n",
    "            output.append(targetId)\n",
    "    return output\n",
    "\n",
    "\n",
    "def readTaskId(pathToFile):\n",
    "    \"\"\"\n",
    "    Reads in a .txt file of a single line of task ids separated by commas. Returns the task ids as a list.\n",
    "    Returns a list of tuples that represents the vertices of a graph.\n",
    "    \"\"\"\n",
    "    with open(pathToFile, \"r\") as file:\n",
    "        content = file.read()\n",
    "    out = content.split(\",\")\n",
    "    out = list(map(str,out))\n",
    "    out = [(x,) for x in out]\n",
    "    return out\n",
    "taskId = readTaskId('task_ids.txt')\n",
    "\n",
    "def readRelations(pathToFile):\n",
    "    \"\"\"\n",
    "    Reads in a .txt file that represents the relationships between the task ids. They are delimited by '->'.\n",
    "    Returns a list of tuples that represents directed edges of a graph.\n",
    "    \"\"\"\n",
    "    with open(pathToFile, \"r\") as file:\n",
    "        content = file.read()\n",
    "    out = content.splitlines()\n",
    "    out = [i.split('->', 1) for i in out]\n",
    "    out = list(map(tuple, out)) \n",
    "    return out\n",
    "\n",
    "\n",
    "SUBMIT_ARGS = \"--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "\n",
    "conf = pyspark.SparkConf(\"local[4]\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName('eqWorksAssessment').getOrCreate()\n",
    "\n",
    "# set schema for incoming data\n",
    "requests_schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"TimeStamp\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Province\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Latitude\", DoubleType(), True),\n",
    "    StructField(\"Longitude\", DoubleType(), True)])\n",
    "poi_schema = StructType([\n",
    "    StructField(\"POIID\", StringType(), True),\n",
    "    StructField(\"poi_lat\", DoubleType(), True),\n",
    "    StructField(\"poi_long\", DoubleType(), True)])\n",
    "\n",
    "dataSample = spark.read.csv(\"data/DataSample.csv\",header=True,schema=requests_schema).coalesce(12)\n",
    "poi = spark.read.csv(\"data/POIList.csv\",header=True,schema=poi_schema).cache()\n",
    "poi.registerTempTable('poi')\n",
    "target_cols = [\"TimeStamp\",\"Country\",\"Province\",\"City\",\"Latitude\",\"Longitude\"]\n",
    "requests = dataSample.dropDuplicates(target_cols).coalesce(12).cache()\n",
    "\n",
    "print(\"Question 1.\")\n",
    "print(\"Count of data sample before removing duplicates: \" + str(dataSample.count()))\n",
    "print(\"Count of data sample after removing duplicates: \" + str(requests.count()))\n",
    "\n",
    "print(\"Question 2\")\n",
    "print(\"Since we later need to 'draw' a circle, I have chosen to implement the euclidean distance over the haversine distance.\")\n",
    "# Resource: https://stackoverflow.com/questions/60086180/pyspark-how-to-apply-a-python-udf-to-pyspark-dataframe-columns\n",
    "combined = requests.crossJoin(poi).cache()\n",
    "combined = combined.withColumn(\"longitude_part\", (col(\"poi_long\") - col(\"Longitude\")) ** 2) \\\n",
    ".withColumn(\"latitude_part\", (col(\"poi_lat\") - col(\"Latitude\")) ** 2) \\\n",
    ".withColumn(\"euclidean_distance\", sqrt(col(\"latitude_part\") + col(\"longitude_part\"))) \\\n",
    ".drop(\"longitude_part\", \"latitude_part\") # Implementation of euclidean distance\n",
    "w = Window.partitionBy([\"ID\",\"TimeStamp\",\"Country\",\"Province\",\"City\",\"Latitude\",\"Longitude\"]).orderBy('euclidean_distance')\n",
    "combined = combined.withColumn(\"rn\", row_number().over(w)).where(col('rn') == 1).drop(\"rn\").cache() # Select the minimum distance and assign to a POI\n",
    "combined = combined.dropDuplicates(target_cols).coalesce(12).cache()\n",
    "combined.show()\n",
    "combined.registerTempTable('combined')\n",
    "print(\"Check: There are \" + str(combined.count()) + \" entries in this dataframe. Then it means we still don't have duplicates after the cross join.\")\n",
    "\n",
    "# Question 3\n",
    "print(\"Question 3\")\n",
    "print(\"The average and standard deviation between the POI to each of its assigned requests is as follows.\")\n",
    "print(\"Note that POI1 and POI2 are in the same exact location, and in this run POI2 have no requests assigned to it and have null as values.\")\n",
    "print(\"Assumption: Given a POI with duplicate geographical data points, all requests will route to one of the POI with that data points\")\n",
    "query3 = \"SELECT poi.POIID, average_distance, stddev_samp_distance, radius, density FROM (SELECT POIID,\\\n",
    "        avg(euclidean_distance) as average_distance,\\\n",
    "        stddev_samp(euclidean_distance) as stddev_samp_distance,\\\n",
    "        max(euclidean_distance) as radius,\\\n",
    "        count(ID)/pow(max(euclidean_distance)*pi(),2) as density\\\n",
    "        FROM combined GROUP BY POIID) as combined RIGHT JOIN poi ON poi.POIID = combined.POIID\"\n",
    "q3 = sqlContext.sql(query3)\n",
    "q3.show()\n",
    "\n",
    "print(\"Question 4a #1\")\n",
    "print(\"Providing a mathematical model to map the popularity of POIs in a scale of [-10,10].\")\n",
    "print(\"This solution is inspired by the boxplot way of visualizing data where we can see the centrality of the data based on the median which is not sensitive to outliers.\")\n",
    "print(\"In essence we have two calculated fields which is the count of cities over province and density for each POI.\")\n",
    "print(\"Number of countries was ommited because I the data comes from Canada only (Validated in bonus.txt).\")\n",
    "print(\"Then we calculate the percentile rank over each of the rows. The percentile method will take care of the new requests coming in, that can be outliers, since it depends on the data itself and will scale accordingly.\")\n",
    "print(\"We sum the percentiles of the calculated field (each calculated field have ranges [0,1]), and we get a data value of range [0,2]. We multiply by 10 and subtract by 10 to get the desired scale of [-10,10].\")\n",
    "print(\"Note that POI2 is considered as popularity -10 as expected since no requests are ever routed there in this iteration.\")\n",
    "print(\"Equations:\")\n",
    "print(\"city_over_province = count(city)/count(province)\")\n",
    "print(\"density = requests / area\")\n",
    "print(\"Note: In order to scale this process perhaps recalculating the popularity over a period of time daily or 4x a day depending on the density of requests of the time and day.\")\n",
    "print(\"popularity = (percent_rank(city_over_province) + percent_rank(density))*10 - 10\")\n",
    "# Link to inspiring idea: https://www.sqlshack.com/calculate-sql-percentile-using-the-sql-server-percent_rank-function/\")\n",
    "query4a1 = \"SELECT * ,\\\n",
    "            (city_over_province_pct_rank + density_pct_rank)*10 - 10 as popularity \\\n",
    "            FROM (SELECT POIID,\\\n",
    "                PERCENT_RANK()\\\n",
    "                    OVER(ORDER BY count_city_over_province) AS city_over_province_pct_rank,\\\n",
    "                PERCENT_RANK()\\\n",
    "                    OVER(ORDER BY density) AS density_pct_rank\\\n",
    "                FROM (SELECT poi.POIID, count_city_over_province, density FROM\\\n",
    "                    (SELECT POIID,\\\n",
    "                        COUNT(Distinct City)/Count(Distinct Province) as count_city_over_province, \\\n",
    "                        count(ID)/pow(max(euclidean_distance)*pi(),2) as density\\\n",
    "                    FROM combined GROUP BY POIID) as combined RIGHT JOIN poi ON poi.POIID = combined.POIID) as combined)\"\n",
    "q4a1 = sqlContext.sql(query4a1)\n",
    "q4a1.show()\n",
    "\n",
    "# Need to find the minimum amount of tasks that are ordered based on the dag's topology.\n",
    "# We would do a postorder traversal when printing the tasks. I.e. dependent task then parent task\n",
    "# \n",
    "print(\"Question 4b\")\n",
    "print(\"We are using the graphframes package that is better suited to work with graphs.\")\n",
    "print(\"Need to find the minimum amount of tasks that are ordered based on the dag's topology.\")\n",
    "print(\"We begin by finding the shortest path from task 73 to task 36.\")\n",
    "print(\"Then we would do a postorder traversal when printing the tasks. I.e. dependent task then parent task.\")\n",
    "questionTask = readQuestion(\"question.txt\")\n",
    "relations = readRelations('relations.txt')\n",
    "taskId = readTaskId('task_ids.txt')  \n",
    "vertices = sqlContext.createDataFrame(taskId,['id'])\n",
    "edges = sqlContext.createDataFrame(relations,['src','dst'])\n",
    "g = GraphFrame(vertices,edges).cache()\n",
    "g = g.dropIsolatedVertices().cache()\n",
    "candidate = g.shortestPaths(questionTask[1])\n",
    "# sort by shortest path first we will begin with that\n",
    "candidate = candidate.select(\"id\", \"distances\").select(\"id\", explode(\"distances\")).filter(candidate.id.isin(questionTask[0])).orderBy(\"value\",ascending=True)\n",
    "candidate_first_task = candidate.first().id\n",
    "minimum_path = g.bfs(\"id='\"+candidate_first_task+\"'\",\"id='\"+questionTask[1][0]+\"'\")\n",
    "minimum_path = sc.parallelize(minimum_path.select(\"from.id\",\"v1.id\",\"v2.id\",\"to.id\").first())\n",
    "minimum_path = minimum_path.collect()\n",
    "print(\"This is our shortest path from the task start to task end: \" + str(minimum_path))\n",
    "print(\"We need to find all the dependent tasks in between each task excluding the task start.\")\n",
    "\n",
    "sol = []\n",
    "visited = []\n",
    "visited = findInitialVisited(g,[candidate_first_task])\n",
    "sol = sol + [candidate_first_task]\n",
    "for i in range(1, len(minimum_path)):\n",
    "    new_sol, new_visited = findPrerequisiteTask(g,[minimum_path[i]],visited)\n",
    "    sol = sol + new_sol\n",
    "    visited = new_visited\n",
    "print(\"These are all the tasks needed to be completed starting from from start to end: \" + str(sol))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-flash",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
